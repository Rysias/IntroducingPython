{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911e631a-74e8-44d7-861b-5686cf7941d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Week 3. Day 1. Exercises from Chapter 7 of FSStDS. \n",
    "## Fundamentals of Social Data Science. MT 2022\n",
    "\n",
    "Within your NEW study pod discuss the following questions. Please submit an individual assignment by 12:30pm Tuesday, October 25, 2022 on Canvas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02f81c-adaa-43e9-9978-1b303f44e1c6",
   "metadata": {},
   "source": [
    "# Exercise 1. How busy is twitter now? \n",
    "\n",
    "Using the `counts` endpoint, compare the chatter around three British politicians, Penny Mordunt, Rishi Sunak, and Boris Johnson. Use the name of the politicians in separate queries:\n",
    "1. Which politician had the most mentions overall?\n",
    "2. For each politician, what was the hour of peak mentions? Was it the same for each person? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf84c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def read_json(filepath: str) -> dict:\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# authentication\n",
    "config = read_json(\"../twitter_config.json\")\n",
    "headers = {\"Authorization\": f\"Bearer {config['bearer_token']}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6610c567-3d61-4842-a6bd-240102b14edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rishi Sunak has the most mentions overall with 39671 tweets\n",
      "name            hour\n",
      "Rishi Sunak     13      687.571429\n",
      "                14      645.000000\n",
      "                15      483.714286\n",
      "Boris Johnson   20      146.857143\n",
      "                18      127.857143\n",
      "                19      124.000000\n",
      "Penny Mordaunt  15       39.571429\n",
      "                16       32.571429\n",
      "                11       30.571429\n",
      "Name: tweet_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Answer 1 below here\n",
    "\n",
    "\n",
    "POLITICIAN_DICT = {\n",
    "    \"Boris Johnson\": \"@BorisJohnson\",\n",
    "    \"Penny Mordaunt\": \"@PennyMordaunt\",\n",
    "    \"Rishi Sunak\": \"@RishiSunak\",\n",
    "}\n",
    "\n",
    "count_url = \"https://api.twitter.com/2/tweets/counts/recent\"\n",
    "\n",
    "\n",
    "def create_politician_query(pol_name: str) -> str:\n",
    "    return f\"{POLITICIAN_DICT[pol_name]} {pol_name}\"\n",
    "\n",
    "def get_tweet_count(pol_name: str) -> int:\n",
    "    query = create_politician_query(pol_name)\n",
    "    params = {\"query\": query}\n",
    "    response = requests.get(count_url, headers=headers, params=params)\n",
    "    return response.json()\n",
    "\n",
    "pol_results = {pol_name: get_tweet_count(pol_name) for pol_name in POLITICIAN_DICT}\n",
    "\n",
    "\n",
    "# Find the one with most mentions overall\n",
    "max_pol = max(pol_results, key=lambda x: pol_results[x][\"meta\"][\"total_tweet_count\"])\n",
    "print(f\"{max_pol} has the most mentions overall with {pol_results[max_pol]['meta']['total_tweet_count']} tweets\")\n",
    "# Find hour of peak mentions #\n",
    "# Normalize data into dataframe\n",
    "df = pd.concat([pd.json_normalize(pol_results[pol_name][\"data\"]).assign(name=pol_name) for pol_name in pol_results])\n",
    "df[\"end\"] = pd.to_datetime(df[\"end\"])\n",
    "df[\"start\"] = pd.to_datetime(df[\"start\"])\n",
    "df[\"hour\"] = df[\"start\"].dt.hour\n",
    "df[\"tweet_count\"] = df[\"tweet_count\"].astype(int)\n",
    "# Find the hour with the most mentions\n",
    "print(df.groupby([\"name\", \"hour\"])[\"tweet_count\"].mean().sort_values(ascending=False).groupby(\"name\").head(3))\n",
    "\n",
    "# Answer 1 above here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fea753-eb5d-488d-84ec-5c6b147b9228",
   "metadata": {},
   "source": [
    "# Exercise 2. Tweets across the world\n",
    "\n",
    "Select a query to download 100 tweets. Include the locations entities. Can you find a query where no single country makes up more than 50% of the locations? How can you automate this using a bank of queries? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb41787",
   "metadata": {},
   "source": [
    "**UPDATE**: WUHUUU, I got academic access so now everything works! Shout out to Khyati for figuring out the magic passwords \n",
    "\n",
    "**NOTE**: This was a total pain to do, and I have wept many tears in not getting this to work. Summa summarum: it is possible to filter with the `has:geo`-query for the premium API. For us mere mortals waiting for recognition from the Twitter gods, there is not much to do as very few (~1 in 100) of the tweets actually has a geo-tag. However, if you masters of SDS figure something out for the free tier, I'm all ears!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2fd6f0-50db-4cd7-a1e2-25efab17dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2 below here \n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "international_query = \"football has:geo\"\n",
    "params = {\"query\": international_query, \"tweet.fields\": \"text,author_id,created_at,geo\",\"expansions\":\"geo.place_id\", \"place.fields\": \"id,country_code\", \"max_results\": 100}\n",
    "response = requests.get(search_url, headers=headers, params=params)\n",
    "\n",
    "\n",
    "# Answer 2 above here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8855bf44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country_code\n",
       "US              0.46\n",
       "GB              0.17\n",
       "SA              0.12\n",
       "IN              0.03\n",
       "BR              0.03\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = pd.json_normalize(response.json()[\"includes\"][\"places\"])\n",
    "tweets = pd.json_normalize(response.json()[\"data\"])\n",
    "country_counts = pd.merge(tweets, locations, left_on=\"geo.place_id\", right_on=\"id\")[[\"text\", \"full_name\", \"country_code\"]].value_counts(subset=[\"country_code\"])\n",
    "country_frequencies = country_counts / country_counts.sum()\n",
    "country_frequencies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043fb63-ce8b-4cc3-ab76-074ac13cbc8e",
   "metadata": {},
   "source": [
    "# Exercise 3. Comparing `praw` and `psaw`\n",
    "\n",
    "Reddit mods have considerable power over their subreddits. In /r/Science they are notoriously aggressive in their deletion of non-scientific comments. \n",
    "\n",
    "PushShift is an archive of reddit data. You can access this archive using `psaw` which is otherwise very similar to `praw`. Go to /r/science and select a story that over two days old. Compare the comments that you receive from `praw` and `psaw`. Do they have similar: \n",
    "- counts of comments?\n",
    "- counts of comments marked deleted?\n",
    "- upvote scores on the comments? \n",
    "\n",
    "Posit what might account for the difference. Reflect on how this could intervene in making claims about activity on Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2926da7a-6a8e-4ced-b82f-8217171e4f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments (PRAW): 2796\n",
      "Number of comments (PSAW): 1\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3 code part below here\n",
    "import praw\n",
    "import psaw\n",
    "\n",
    "post_id = \"y2439s\"\n",
    "reddit_config = read_json(\"../reddit_config.json\")\n",
    "reddit = praw.Reddit(\n",
    "    client_id=reddit_config[\"client_id\"],\n",
    "    client_secret=reddit_config[\"client_secret\"],\n",
    "    user_agent=\"SDS Scraper Jonathan\",\n",
    ")\n",
    "\n",
    "# get number of comments (PRAW)\n",
    "submission = reddit.submission(id=post_id)\n",
    "print(f\"Number of comments (PRAW): {submission.num_comments}\")\n",
    "\n",
    "# get number of comments (PSAW)\n",
    "api = psaw.PushshiftAPI()\n",
    "comments = next(api.search_submissions(ids=[post_id]))\n",
    "num_comments = comments.num_comments\n",
    "print(f\"Number of comments (PSAW): {num_comments}\")\n",
    "\n",
    "# Exercise 3 code part above here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704ffdc-d879-4b41-b66f-cc1ace0d0f39",
   "metadata": {},
   "source": [
    "## Exercise 3 reflections below here \n",
    "PSAW seems to be way, way, WAY out of date. I honestly don't knwo why anyone would ever use it for any dynamic data (upvotes, comments etc). D- from here!\n",
    "\n",
    "## Exercise 3 reflections above here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534237ab-618f-40bb-becf-8d28d1c43367",
   "metadata": {},
   "source": [
    "# Exercise 4 - Comparing comments \n",
    "(in case PushShift isn't working for you)\n",
    "\n",
    "People can submit the same link to multiple subreddits. You can find out where else a link was submitted on Reddit by xx. \n",
    "\n",
    "Select a story that has been submitted to at least two subreddits. Compare the comments for each of these:\n",
    "- Which subreddit has more comments? Would this have been unexpected? Why?\n",
    "- Are there any overlap in the users who comment on these stories across the multiple subreddits?\n",
    "- Summarise the scores of the comments versus the story (`data.ups`). Is the ratio of the top comment score to the ups the same? \n",
    "- Which story has more comments that have a score below zero? As a percentage? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7206724d-527c-4999-98da-fab9707fe13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments for y6rsz2 (subreddit: FoodPorn): 219\n",
      "Number of comments for y6rubo (subreddit: food): 682\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4 below here\n",
    "link1 = \"y6rsz2\"\n",
    "link2 = \"y6rubo\"\n",
    "\n",
    "# which has more comments?\n",
    "submission1 = reddit.submission(id=link1)\n",
    "submission2 = reddit.submission(id=link2)\n",
    "print(f\"Number of comments for {link1} (subreddit: {submission1.subreddit.display_name}): {submission1.num_comments}\")\n",
    "print(f\"Number of comments for {link2} (subreddit: {submission2.subreddit.display_name}): {submission2.num_comments}\")\n",
    "\n",
    "# Exercise 4 above here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3cd7932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of commentors in both subreddits: 1\n",
      "Commentors in both subreddits: {'AlbionReturns'}\n",
      "OP of both posts: Jackpot09\n"
     ]
    }
   ],
   "source": [
    "submission2.comments.replace_more(limit=0)\n",
    "sub1_commentors = {comment.author.name for comment in submission1.comments}\n",
    "sub2_commentors = {comment.author.name for comment in submission2.comments if comment.author is not None}\n",
    "\n",
    "commment_intersection = sub1_commentors.intersection(sub2_commentors)\n",
    "intersection_size = len(commment_intersection)\n",
    "print(f\"Number of commentors in both subreddits: {intersection_size}\")\n",
    "print(f\"Commentors in both subreddits: {commment_intersection}\")\n",
    "print(f\"OP of both posts: {submission1.author.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fba09a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top comment score for y6rsz2 (subreddit: FoodPorn): 289 (3.78%)\n",
      "Top comment score for y6rubo (subreddit: food): 1818 (13.77%)\n"
     ]
    }
   ],
   "source": [
    "def get_top_comment_score(submission: praw.models.Submission) -> int:\n",
    "    return max(submission.comments, key=lambda x: x.score).score\n",
    "\n",
    "top_comment_sub1 = get_top_comment_score(submission1)\n",
    "top_comment_sub2 = get_top_comment_score(submission2)\n",
    "top_ratio_sub1 = top_comment_sub1 / submission1.score\n",
    "top_ratio_sub2 = top_comment_sub2 / submission2.score\n",
    "print(f\"Top comment score for {link1} (subreddit: {submission1.subreddit.display_name}): {top_comment_sub1} ({top_ratio_sub1:.2%})\")\n",
    "print(f\"Top comment score for {link2} (subreddit: {submission2.subreddit.display_name}): {top_comment_sub2} ({top_ratio_sub2:.2%})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ae642f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative score comments for y6rsz2 (subreddit: FoodPorn): 13\n",
      "Number of negative score comments for y6rubo (subreddit: food): 26\n"
     ]
    }
   ],
   "source": [
    "## BELOW ZERO ##\n",
    "from typing import List\n",
    "def get_negative_score_comments(submission: praw.models.Submission) -> List[praw.models.Comment]:\n",
    "    return [comment for comment in submission.comments if comment.score < 0]\n",
    "\n",
    "num_neg_comments_sub1 = len(get_negative_score_comments(submission1))\n",
    "num_neg_comments_sub2 = len(get_negative_score_comments(submission2))\n",
    "print(f\"Number of negative score comments for {link1} (subreddit: {submission1.subreddit.display_name}): {num_neg_comments_sub1}\")\n",
    "print(f\"Number of negative score comments for {link2} (subreddit: {submission2.subreddit.display_name}): {num_neg_comments_sub2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ce678",
   "metadata": {},
   "source": [
    "### General notes\n",
    "It seems that the more popular submission (the one of r/food) in general has more extreme values in all regards. This makes sense from a statistical perspective as the extreme values of larger distributions tend to be more extreme. It also shows the advantages of x-posting when farming karma :))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sdspython')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "60612d6d4782d833a44515694e4bccfae5bb8e3209e7562cc1fda167f2433927"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
