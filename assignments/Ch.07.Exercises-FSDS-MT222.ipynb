{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911e631a-74e8-44d7-861b-5686cf7941d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Week 3. Day 1. Exercises from Chapter 7 of FSStDS. \n",
    "## Fundamentals of Social Data Science. MT 2022\n",
    "\n",
    "Within your NEW study pod discuss the following questions. Please submit an individual assignment by 12:30pm Tuesday, October 25, 2022 on Canvas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02f81c-adaa-43e9-9978-1b303f44e1c6",
   "metadata": {},
   "source": [
    "# Exercise 1. How busy is twitter now? \n",
    "\n",
    "Using the `counts` endpoint, compare the chatter around three British politicians, Penny Mordunt, Rishi Sunak, and Boris Johnson. Use the name of the politicians in separate queries:\n",
    "1. Which politician had the most mentions overall?\n",
    "2. For each politician, what was the hour of peak mentions? Was it the same for each person? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6610c567-3d61-4842-a6bd-240102b14edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rishi Sunak has the most mentions overall with 21468 tweets\n",
      "name            hour\n",
      "Rishi Sunak     13      692.142857\n",
      "                14      649.000000\n",
      "                15      487.571429\n",
      "Boris Johnson   20      148.714286\n",
      "                18      130.285714\n",
      "                19      125.714286\n",
      "Penny Mordaunt  17       48.142857\n",
      "                18       42.571429\n",
      "                20       42.428571\n",
      "Name: tweet_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Answer 1 below here\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def read_json(filepath: str) -> dict:\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "POLITICIAN_DICT = {\n",
    "    \"Boris Johnson\": \"@BorisJohnson\",\n",
    "    \"Penny Mordaunt\": \"@PennyMordaunt\",\n",
    "    \"Rishi Sunak\": \"@RishiSunak\",\n",
    "}\n",
    "\n",
    "count_url = \"https://api.twitter.com/2/tweets/counts/recent\"\n",
    "\n",
    "# authentication\n",
    "config = read_json(\"../twitter_config.json\")\n",
    "headers = {\"Authorization\": f\"Bearer {config['bearer_token']}\"}\n",
    "\n",
    "def create_politician_query(pol_name: str) -> str:\n",
    "    return f\"{POLITICIAN_DICT[pol_name]} {pol_name}\"\n",
    "\n",
    "def get_tweet_count(pol_name: str) -> int:\n",
    "    query = create_politician_query(pol_name)\n",
    "    params = {\"query\": query}\n",
    "    response = requests.get(count_url, headers=headers, params=params)\n",
    "    return response.json()\n",
    "\n",
    "pol_results = {pol_name: get_tweet_count(pol_name) for pol_name in POLITICIAN_DICT}\n",
    "\n",
    "\n",
    "# Find the one with most mentions overall\n",
    "max_pol = max(pol_results, key=lambda x: pol_results[x][\"meta\"][\"total_tweet_count\"])\n",
    "print(f\"{max_pol} has the most mentions overall with {pol_results[max_pol]['meta']['total_tweet_count']} tweets\")\n",
    "# Find hour of peak mentions #\n",
    "# Normalize data into dataframe\n",
    "df = pd.concat([pd.json_normalize(pol_results[pol_name][\"data\"]).assign(name=pol_name) for pol_name in pol_results])\n",
    "df[\"end\"] = pd.to_datetime(df[\"end\"])\n",
    "df[\"start\"] = pd.to_datetime(df[\"start\"])\n",
    "df[\"hour\"] = df[\"start\"].dt.hour\n",
    "df[\"tweet_count\"] = df[\"tweet_count\"].astype(int)\n",
    "# Find the hour with the most mentions\n",
    "print(df.groupby([\"name\", \"hour\"])[\"tweet_count\"].mean().sort_values(ascending=False).groupby(\"name\").head(3))\n",
    "\n",
    "# Answer 1 above here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fea753-eb5d-488d-84ec-5c6b147b9228",
   "metadata": {},
   "source": [
    "# Exercise 2. Tweets across the world\n",
    "\n",
    "Select a query to download 100 tweets. Include the locations entities. Can you find a query where no single country makes up more than 50% of the locations? How can you automate this using a bank of queries? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb41787",
   "metadata": {},
   "source": [
    "**NOTE**: This was a total pain to do, and I have wept many tears in not getting this to work. Summa summarum: it is possible to filter with the `has:geo`-query for the premium API. For us mere mortals waiting for recognition from the Twitter gods, there is not much to do as very few (~1 in 100) of the tweets actually has a geo-tag. However, if you masters of SDS figure something out for the free tier, I'm all ears!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ad2fd6f0-50db-4cd7-a1e2-25efab17dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2 below here \n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "international_query = \"kpop has:geo\"\n",
    "params = {\"query\": international_query, \"tweet.fields\": \"text,author_id,created_at,geo\",\"expansions\":\"geo.place_id\", \"place.fields\": \"country\", \"max_results\": 100}\n",
    "response = requests.get(search_url, headers=headers, params=params)\n",
    "\n",
    "\n",
    "# Answer 2 above here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d275f325",
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Forbidden\nWhen authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [118], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m client \u001b[39m=\u001b[39m tweepy\u001b[39m.\u001b[39mClient(config[\u001b[39m\"\u001b[39m\u001b[39mbearer_token\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[39m# verify credentials\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m response_test \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49msearch_all_tweets(query\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mkpop has:geo -is:retweet\u001b[39;49m\u001b[39m\"\u001b[39;49m, tweet_fields\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mauthor_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcreated_at\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgeo\u001b[39;49m\u001b[39m\"\u001b[39;49m], expansions\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mgeo.place_id\u001b[39;49m\u001b[39m\"\u001b[39;49m], place_fields\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcountry\u001b[39;49m\u001b[39m\"\u001b[39;49m], max_results\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, )\n",
      "File \u001b[1;32mc:\\Users\\jhr\\Anaconda3\\envs\\sdspython\\lib\\site-packages\\tweepy\\client.py:1145\u001b[0m, in \u001b[0;36mClient.search_all_tweets\u001b[1;34m(self, query, **params)\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[39m\"\"\"search_all_tweets( \\\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[39m    query, *, end_time=None, expansions=None, max_results=None, \\\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[39m    media_fields=None, next_token=None, place_fields=None, \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[39m.. _pagination: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/paginate\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m params[\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m query\n\u001b[1;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m   1146\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m/2/tweets/search/all\u001b[39;49m\u001b[39m\"\u001b[39;49m, params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m   1147\u001b[0m     endpoint_parameters\u001b[39m=\u001b[39;49m(\n\u001b[0;32m   1148\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mend_time\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mexpansions\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmax_results\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmedia.fields\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1149\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mnext_token\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mplace.fields\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mpoll.fields\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1150\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39msince_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msort_order\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstart_time\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtweet.fields\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1151\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39muntil_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39muser.fields\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m   1152\u001b[0m     ), data_type\u001b[39m=\u001b[39;49mTweet\n\u001b[0;32m   1153\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jhr\\Anaconda3\\envs\\sdspython\\lib\\site-packages\\tweepy\\client.py:126\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_request\u001b[39m(\u001b[39mself\u001b[39m, method, route, params\u001b[39m=\u001b[39m{}, endpoint_parameters\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    123\u001b[0m                   json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, user_auth\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    124\u001b[0m     request_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_params(params, endpoint_parameters)\n\u001b[1;32m--> 126\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(method, route, params\u001b[39m=\u001b[39;49mrequest_params,\n\u001b[0;32m    127\u001b[0m                             json\u001b[39m=\u001b[39;49mjson, user_auth\u001b[39m=\u001b[39;49muser_auth)\n\u001b[0;32m    129\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_type \u001b[39mis\u001b[39;00m requests\u001b[39m.\u001b[39mResponse:\n\u001b[0;32m    130\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\jhr\\Anaconda3\\envs\\sdspython\\lib\\site-packages\\tweepy\\client.py:99\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[39mraise\u001b[39;00m Unauthorized(response)\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m403\u001b[39m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mraise\u001b[39;00m Forbidden(response)\n\u001b[0;32m    100\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m404\u001b[39m:\n\u001b[0;32m    101\u001b[0m     \u001b[39mraise\u001b[39;00m NotFound(response)\n",
      "\u001b[1;31mForbidden\u001b[0m: 403 Forbidden\nWhen authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal."
     ]
    }
   ],
   "source": [
    "# Get the data\n",
    "import tweepy\n",
    "\n",
    "# authentication\n",
    "client = tweepy.Client(config[\"bearer_token\"])\n",
    "# verify credentials\n",
    "response_test = client.search_recent_tweets(query=\"kpop -is:retweet\", tweet_fields=[\"text\", \"author_id\", \"created_at\", \"geo\"], expansions=[\"geo.place_id\"], place_fields=[\"country\"], max_results=100, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0fb3ef25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'place_id': '62f9f7941af63107'}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tweet.geo for tweet in response_test.data if tweet.geo is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043fb63-ce8b-4cc3-ab76-074ac13cbc8e",
   "metadata": {},
   "source": [
    "# Exercise 3. Comparing `praw` and `psaw`\n",
    "\n",
    "Reddit mods have considerable power over their subreddits. In /r/Science they are notoriously aggressive in their deletion of non-scientific comments. \n",
    "\n",
    "PushShift is an archive of reddit data. You can access this archive using `psaw` which is otherwise very similar to `praw`. Go to /r/science and select a story that over two days old. Compare the comments that you receive from `praw` and `psaw`. Do they have similar: \n",
    "- counts of comments?\n",
    "- counts of comments marked deleted?\n",
    "- upvote scores on the comments? \n",
    "\n",
    "Posit what might account for the difference. Reflect on how this could intervene in making claims about activity on Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2926da7a-6a8e-4ced-b82f-8217171e4f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments (PRAW): 2797\n",
      "Number of comments (PSAW): 1\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3 code part below here\n",
    "import praw\n",
    "import psaw\n",
    "\n",
    "post_id = \"y2439s\"\n",
    "reddit_config = read_json(\"../reddit_config.json\")\n",
    "reddit = praw.Reddit(\n",
    "    client_id=reddit_config[\"client_id\"],\n",
    "    client_secret=reddit_config[\"client_secret\"],\n",
    "    user_agent=\"SDS Scraper Jonathan\",\n",
    ")\n",
    "\n",
    "# get number of comments (PRAW)\n",
    "submission = reddit.submission(id=post_id)\n",
    "print(f\"Number of comments (PRAW): {submission.num_comments}\")\n",
    "\n",
    "# get number of comments (PSAW)\n",
    "api = psaw.PushshiftAPI()\n",
    "comments = next(api.search_submissions(ids=[post_id]))\n",
    "num_comments = comments.num_comments\n",
    "print(f\"Number of comments (PSAW): {num_comments}\")\n",
    "\n",
    "# Exercise 3 code part above here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704ffdc-d879-4b41-b66f-cc1ace0d0f39",
   "metadata": {},
   "source": [
    "## Exercise 3 reflections below here \n",
    "PSAW seems to be way, way, WAY out of date. I honestly don't knwo why anyone would ever use it for any dynamic data (upvotes, comments etc). D- from here!\n",
    "\n",
    "## Exercise 3 reflections above here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534237ab-618f-40bb-becf-8d28d1c43367",
   "metadata": {},
   "source": [
    "# Exercise 4 - Comparing comments \n",
    "(in case PushShift isn't working for you)\n",
    "\n",
    "People can submit the same link to multiple subreddits. You can find out where else a link was submitted on Reddit by xx. \n",
    "\n",
    "Select a story that has been submitted to at least two subreddits. Compare the comments for each of these:\n",
    "- Which subreddit has more comments? Would this have been unexpected? Why?\n",
    "- Are there any overlap in the users who comment on these stories across the multiple subreddits?\n",
    "- Summarise the scores of the comments versus the story (`data.ups`). Is the ratio of the top comment score to the ups the same? \n",
    "- Which story has more comments that have a score below zero? As a percentage? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7206724d-527c-4999-98da-fab9707fe13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments for y6rsz2 (subreddit: FoodPorn): 219\n",
      "Number of comments for y6rubo (subreddit: food): 684\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4 below here\n",
    "link1 = \"y6rsz2\"\n",
    "link2 = \"y6rubo\"\n",
    "\n",
    "# which has more comments?\n",
    "submission1 = reddit.submission(id=link1)\n",
    "submission2 = reddit.submission(id=link2)\n",
    "print(f\"Number of comments for {link1} (subreddit: {submission1.subreddit.display_name}): {submission1.num_comments}\")\n",
    "print(f\"Number of comments for {link2} (subreddit: {submission2.subreddit.display_name}): {submission2.num_comments}\")\n",
    "\n",
    "# Exercise 4 above here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c3cd7932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of commentors in both subreddits: 1\n",
      "Commentors in both subreddits: {'AlbionReturns'}\n",
      "OP of both posts: Jackpot09\n"
     ]
    }
   ],
   "source": [
    "submission2.comments.replace_more(limit=0)\n",
    "sub1_commentors = {comment.author.name for comment in submission1.comments}\n",
    "sub2_commentors = {comment.author.name for comment in submission2.comments if comment.author is not None}\n",
    "\n",
    "commment_intersection = sub1_commentors.intersection(sub2_commentors)\n",
    "intersection_size = len(commment_intersection)\n",
    "print(f\"Number of commentors in both subreddits: {intersection_size}\")\n",
    "print(f\"Commentors in both subreddits: {commment_intersection}\")\n",
    "print(f\"OP of both posts: {submission1.author.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4fba09a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top comment score for y6rsz2 (subreddit: FoodPorn): 290 (3.79%)\n",
      "Top comment score for y6rubo (subreddit: food): 1817 (13.77%)\n"
     ]
    }
   ],
   "source": [
    "def get_top_comment_score(submission: praw.models.Submission) -> int:\n",
    "    return max(submission.comments, key=lambda x: x.score).score\n",
    "\n",
    "top_comment_sub1 = get_top_comment_score(submission1)\n",
    "top_comment_sub2 = get_top_comment_score(submission2)\n",
    "top_ratio_sub1 = top_comment_sub1 / submission1.score\n",
    "top_ratio_sub2 = top_comment_sub2 / submission2.score\n",
    "print(f\"Top comment score for {link1} (subreddit: {submission1.subreddit.display_name}): {top_comment_sub1} ({top_ratio_sub1:.2%})\")\n",
    "print(f\"Top comment score for {link2} (subreddit: {submission2.subreddit.display_name}): {top_comment_sub2} ({top_ratio_sub2:.2%})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b2ae642f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative score comments for y6rsz2 (subreddit: FoodPorn): 13\n",
      "Number of negative score comments for y6rubo (subreddit: food): 24\n"
     ]
    }
   ],
   "source": [
    "## BELOW ZERO ##\n",
    "from typing import List\n",
    "def get_negative_score_comments(submission: praw.models.Submission) -> List[praw.models.Comment]:\n",
    "    return [comment for comment in submission.comments if comment.score < 0]\n",
    "\n",
    "num_neg_comments_sub1 = len(get_negative_score_comments(submission1))\n",
    "num_neg_comments_sub2 = len(get_negative_score_comments(submission2))\n",
    "print(f\"Number of negative score comments for {link1} (subreddit: {submission1.subreddit.display_name}): {num_neg_comments_sub1}\")\n",
    "print(f\"Number of negative score comments for {link2} (subreddit: {submission2.subreddit.display_name}): {num_neg_comments_sub2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ce678",
   "metadata": {},
   "source": [
    "### General notes\n",
    "It seems that the more popular submission (the one of r/food) in general has more extreme values in all regards. This makes sense from a statistical perspective as the extreme values of larger distributions tend to be more extreme. It also shows the advantages of x-posting when farming karma :))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sdspython')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "60612d6d4782d833a44515694e4bccfae5bb8e3209e7562cc1fda167f2433927"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
