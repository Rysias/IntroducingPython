{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911e631a-74e8-44d7-861b-5686cf7941d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Week 4. Day 1. Exercises from Chapter 10 of FSStDS. \n",
    "## Fundamentals of Social Data Science. MT 2022\n",
    "\n",
    "Within your study pod discuss the following questions. Please submit an individual assignment by 12:30pm Tuesday, November 1st, 2022 on Canvas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6159a-d2d8-4d47-a900-172e0ceb3fdd",
   "metadata": {},
   "source": [
    "# Refactoring code \n",
    "\n",
    "Chapter 10 gave the example of the Movie Stack Exchange as a file with posts that could be cleaned and imported into Python. The steps taken are sequential and result in a final DataFrame which was pickled. \n",
    "\n",
    "Below we want you to proceed in steps, refactor or rewriting that code until we end up with a script whereby you can take the zipped "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a971bc3-1a39-4973-9693-58a5f5e68c88",
   "metadata": {},
   "source": [
    "# Step 1. \n",
    "**Be able to get from the 7z file to the preferred XML file**\n",
    "\n",
    "In the first step (for which I've provided starter code), you should be able to open a downloaded 7z file representing the archive, export it to a folder under data dir.\n",
    "\n",
    "## Challenge 1. \n",
    "Can you do this with data from the web instead of downloading it first?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1bff34-06cc-451a-8667-e7efc2d004ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer step 1 below here\n",
    "\n",
    "# Note you will likely need to install py7zr through pip \n",
    "# or use an altnernate approach to unpacking such as \n",
    "# pyunpack or libarchive (both of which I found fussy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c72619d-599d-4490-a9fa-49f5b41da94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from py7zr import unpack_7zarchive\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "\n",
    "stack = \"sustainability\"\n",
    "file_name = f\"{stack}.stackexchange.com.7z\"\n",
    "HTML_PATH = f\"https://archive.org/download/stackexchange/{file_name}\"\n",
    "\n",
    "DATA_DIR = Path().cwd().parent / \"data\"\n",
    "export_path = DATA_DIR / f\"{stack}\"\n",
    "\n",
    "if not export_path.exists(): \n",
    "    export_path.mkdir()\n",
    "\n",
    "r = requests.get(HTML_PATH, stream=True)\n",
    "with open(DATA_DIR / file_name, 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd6a17e-a4bd-49a2-b064-2eb17318700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)\n",
    "except shutil.RegistryError:\n",
    "    pass\n",
    "shutil.unpack_archive(DATA_DIR / file_name, export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995baa3-5c4a-48d5-9086-b9b8ce293808",
   "metadata": {},
   "source": [
    "# Step 2. \n",
    "\n",
    "Refactor the code from Chapter 10 of the book into a function that works for the `posts.xml`. That function should take in the base data and then:\n",
    "1. Convert the `int` variables (except where they start or end in `Id`) into integer values.(CHECK)\n",
    "2. Convert the `datetime` variables. (CHECK) \n",
    "3. Convert tags data from `str` into a `list`. (CHECK)\n",
    "4. Create a separate column for `CleanBody` which is the `Body` without HTML. (CHECK) \n",
    "5. Assign the HTML into a column as a list called `ListURL`. (CHECK)\n",
    "6. Pickle that DataFrame with a coherent name, such as `f\"{stack}_Posts_cleaned.xml\"`.\n",
    "\n",
    "Notes: \n",
    "> I say 'in a function', but you might want to have a main function and then helper functions for subprocesses. \n",
    "> You can make this more abstract, but that's coming anyway. Read the exercises below, and then think about this plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a9af143-9354-41c0-b597-c2605043329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Step 2 below here \n",
    "import xmltodict\n",
    "import pandas as pd\n",
    "import functools\n",
    "# import beautifulsoup4\n",
    "import bs4\n",
    "from typing import List, Optional\n",
    "\n",
    "def read_xml_rows(file_path: Path) -> dict:\n",
    "    data_type = file_path.stem.lower()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        xml = f.read()\n",
    "    xml = xmltodict.parse(xml)\n",
    "    rows = xml[data_type][\"row\"]\n",
    "    return rows\n",
    "\n",
    "def read_xml_rows_to_df(file_path: Path) -> pd.DataFrame:\n",
    "    rows = read_xml_rows(file_path)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def id_cols_to_str(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        if col.endswith(\"Id\"):\n",
    "            df[col] = df[col].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_numeric_to_int(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Check if column contains numbers and convert to int\"\"\"\n",
    "    for col in df.columns:\n",
    "        if not col.endswith(\"Id\"):\n",
    "            try:\n",
    "                df[col] = df[col].astype(int)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def convert_date_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        if col.endswith(\"Date\"):\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "    return df\n",
    "\n",
    "def tags_to_list(tags: Optional[str]) -> List[str]:\n",
    "    \"turn <tag1><tag2>... into [tag1, tag2, ...]\"\n",
    "    if not isinstance(tags, str):\n",
    "        return []\n",
    "    return tags.replace(\"><\", \",\").replace(\"<\", \"\").replace(\">\", \"\").split(\",\")\n",
    "\n",
    "def convert_tags(df: pd.DataFrame, tag_col: str=\"@Tags\") -> pd.DataFrame:\n",
    "    df[tag_col] = df[tag_col].apply(tags_to_list)\n",
    "    return df\n",
    "\n",
    "def get_url_from_html(html: str) -> List[str]:\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    return [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n",
    "\n",
    "def create_url_col(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"@ListURLs\"] = df[\"@Body\"].apply(get_url_from_html)\n",
    "    return df\n",
    "\n",
    "def strip_html(text: str) -> str:\n",
    "    return bs4.BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "def strip_html_df(df: pd.DataFrame, text_col: str=\"@Body\") -> pd.DataFrame:\n",
    "    df[\"@CleanBody\"] = df[text_col].apply(strip_html)\n",
    "    return df\n",
    "\n",
    "def remove_adds_from_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove '@' from the column names\"\"\"\n",
    "    df.columns = [col.replace(\"@\", \"\") for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def pickle_df(df: pd.DataFrame, file_path: Path) -> None:\n",
    "    df.to_pickle(file_path)\n",
    "\n",
    "\n",
    "def compose(*funcs):\n",
    "    def inner(arg):\n",
    "        return functools.reduce(lambda x, f: f(x), funcs, arg)\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "40dd1bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jhr\\Anaconda3\\envs\\sdspython\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jhr\\Anaconda3\\envs\\sdspython\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    pipeline = compose(\n",
    "        read_xml_rows_to_df, \n",
    "        id_cols_to_str, \n",
    "        convert_numeric_to_int, \n",
    "        convert_date_cols, \n",
    "        convert_tags, \n",
    "        strip_html_df, \n",
    "        create_url_col,\n",
    "        remove_adds_from_cols,\n",
    "    )\n",
    "    posts = pipeline(DATA_DIR / f\"{stack}/Posts.xml\")\n",
    "    # save to pickle\n",
    "    pickle_df(posts, DATA_DIR / f\"{stack}_Posts_cleaned.pkl\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd063980-d98e-4550-b195-2d8e098e6a1a",
   "metadata": {},
   "source": [
    "# Step 3. \n",
    "\n",
    "Parameterise the function. Depending on how you created the function above, you might have hard coded the names of the columns from the `Posts.xml` data. This time, make a parameter for the specific schema that you are going to use to convert the data. The schema should be stored as JSON and have the type of XML file as a key, with the value being a dictionary for the column names and the conversion, such as the following: \n",
    "\n",
    "~~~ Python\n",
    "[\"Posts\": {\"Id\":none,\n",
    "           \"PostTypeID\":none,...,\n",
    "           \"Tags\":[\"str\",\"list\"],\n",
    "           \"AnswerCount\":\"int\",\n",
    "           \"Body\":[\"str\",\"CleanHTML\",\"listHTML\"]},\n",
    " \"Users\": {\"Id\":none,\n",
    "           \"Reputation\":\"int\", ...}\n",
    "~~~\n",
    "\n",
    "So, now this time, the main function should read in the Schema from file, select the right table type (such as \"Posts\") and then return (or export to pickle) a DataFrame with the same naming conventions as above.   \n",
    "\n",
    "Full schema available here: https://i.stack.imgur.com/AyIkW.png\n",
    "\n",
    "> Note that this schema or question does not make assumptions about what you will do with None values, but it is encouraged for you to consider whether to convert to None, \"\", pd.NA, np.NAN, depending on context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "82483988-d134-46e9-875c-37233ac89e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhr\\AppData\\Local\\Temp\\ipykernel_1260\\1162537027.py:29: FutureWarning: Passing a dict as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  return df[schema]\n",
      "C:\\Users\\jhr\\AppData\\Local\\Temp\\ipykernel_1260\\1162537027.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(int, errors=\"ignore\")\n",
      "C:\\Users\\jhr\\AppData\\Local\\Temp\\ipykernel_1260\\1162537027.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Answer Step 3 below here \n",
    "import json\n",
    "\n",
    "def read_json(file_path: Path) -> dict:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "schema_file = read_json(Path(\"../stackexchange_schema.json\"))\n",
    "\n",
    "def convert_int_cols(df: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
    "    int_cols = [col for col, dtype in schema.items() if dtype == \"int\"]\n",
    "    for col in int_cols:\n",
    "        df[col] = df[col].astype(int, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "def convert_string_cols(df: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
    "    str_cols = [col for col, dtype in schema.items() if dtype == \"str\"]\n",
    "    for col in str_cols:\n",
    "        df[col] = df[col].astype(str)\n",
    "    return df\n",
    "\n",
    "def convert_date_cols(df: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
    "    date_cols = [col for col, dtype in schema.items() if dtype == \"datetime\"]\n",
    "    for col in date_cols:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    return df\n",
    "\n",
    "def choose_cols(df: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
    "    return df[schema]\n",
    "\n",
    "def add_taglist(df: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
    "    if not \"Tags\" in schema:\n",
    "        return df\n",
    "    elif \"TagList\" not in schema[\"Tags\"]:\n",
    "        return df\n",
    "    df[\"TagList\"] = df[\"Tags\"].apply(tags_to_list)\n",
    "    return df\n",
    "\n",
    "def clean_html(df: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
    "    if not \"Body\" in schema:\n",
    "        return df\n",
    "    elif \"CleanHTML\" not in schema[\"Body\"]:\n",
    "        return df\n",
    "    df[\"CleanHTML\"] = df[\"Body\"].apply(strip_html)\n",
    "    return df\n",
    "\n",
    "def add_url_list(df: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
    "    if not \"Body\" in schema:\n",
    "        return df\n",
    "    elif \"ListURL\" not in schema[\"Body\"]:\n",
    "        return df\n",
    "    df[\"ListURL\"] = df[\"Body\"].apply(get_url_from_html)\n",
    "    return df\n",
    "\n",
    "def process_file(file_path: Path) -> None:\n",
    "    data_type = file_path.stem\n",
    "    schema = schema_file[data_type]\n",
    "    \n",
    "    basic_pipeline = compose(\n",
    "        read_xml_rows_to_df,\n",
    "        remove_adds_from_cols,\n",
    "        functools.partial(choose_cols, schema=schema),\n",
    "        functools.partial(convert_int_cols, schema=schema),\n",
    "        functools.partial(convert_string_cols, schema=schema),\n",
    "        functools.partial(convert_date_cols, schema=schema),\n",
    "    )\n",
    "    post_pipeline = compose(\n",
    "        basic_pipeline,\n",
    "        functools.partial(add_taglist, schema=schema),\n",
    "        functools.partial(clean_html, schema=schema),\n",
    "        functools.partial(add_url_list, schema=schema),\n",
    "    )\n",
    "\n",
    "    if data_type == \"Posts\":\n",
    "        df = post_pipeline(file_path)\n",
    "    elif data_type == \"Users\":\n",
    "        df = basic_pipeline(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown data type: {data_type}\")\n",
    "    pickle_df(df, DATA_DIR / f\"{stack}_{data_type}_cleaned2.pkl\")\n",
    "\n",
    "process_file(DATA_DIR / f\"{stack}/Users.xml\")\n",
    "df = pd.read_pickle(DATA_DIR / f\"{stack}_Users_cleaned2.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b515c26-ff72-4305-9fe5-c43d21aa7231",
   "metadata": {},
   "source": [
    "# Step 4. A wide parameter. \n",
    "\n",
    "Create a means (either with a parameter in the original file or using a separate file) in order to create a long table from one of the wide tables. That is, it should take the column that is used for the long data (which we assume would be a list of values) and then `explode()` the table. By default, it should only explode the selected column. You should be able to pass it a list of additional columns that will also appear in the exploded data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2bd45-c7ae-444e-a391-9c733e7d32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Step 4 below here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b93c31-e261-459d-b548-d3809b1b95af",
   "metadata": {},
   "source": [
    "# Challenge #2. \n",
    "_(example code will not be provided)_\n",
    "\n",
    "Recall that we downloaded the data from the Internet Archive. That main URL has a list of all of the Stack Exchanges and their sizes. \n",
    "\n",
    "Can we use this data in order to present a list of Stack Exchanges and then have the user select which one to first download instead of linking directly? \n",
    "\n",
    "Explore this as well as some packages for providing progress bars for the download process. Package all the code up in a script that allows the user to select which Exchange and then receive the resulting preferred tables as .pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92e4e6-89ec-43ba-87da-6d64286e4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Challenge 2 below here \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sdspython')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "60612d6d4782d833a44515694e4bccfae5bb8e3209e7562cc1fda167f2433927"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
