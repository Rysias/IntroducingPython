{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911e631a-74e8-44d7-861b-5686cf7941d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Week 4. Day 1. Exercises from Chapter 10 of FSStDS. \n",
    "## Fundamentals of Social Data Science. MT 2022\n",
    "\n",
    "Within your study pod discuss the following questions. Please submit an individual assignment by 12:30pm Tuesday, November 1st, 2022 on Canvas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6159a-d2d8-4d47-a900-172e0ceb3fdd",
   "metadata": {},
   "source": [
    "# Refactoring code \n",
    "\n",
    "Chapter 10 gave the example of the Movie Stack Exchange as a file with posts that could be cleaned and imported into Python. The steps taken are sequential and result in a final DataFrame which was pickled. \n",
    "\n",
    "Below we want you to proceed in steps, refactor or rewriting that code until we end up with a script whereby you can take the zipped "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a971bc3-1a39-4973-9693-58a5f5e68c88",
   "metadata": {},
   "source": [
    "# Step 1. \n",
    "**Be able to get from the 7z file to the preferred XML file**\n",
    "\n",
    "In the first step (for which I've provided starter code), you should be able to open a downloaded 7z file representing the archive, export it to a folder under data dir.\n",
    "\n",
    "## Challenge 1. \n",
    "Can you do this with data from the web instead of downloading it first?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1bff34-06cc-451a-8667-e7efc2d004ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer step 1 below here\n",
    "\n",
    "# Note you will likely need to install py7zr through pip \n",
    "# or use an altnernate approach to unpacking such as \n",
    "# pyunpack or libarchive (both of which I found fussy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c72619d-599d-4490-a9fa-49f5b41da94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from py7zr import unpack_7zarchive\n",
    "import shutil\n",
    "\n",
    "\n",
    "stack = \"movies\"\n",
    "file_name = f\"{stack}.stackexchange.com.7z\"\n",
    "HTML_PATH = f\"https://archive.org/download/stackexchange/{file_name}\"\n",
    "\n",
    "DATA_DIR = Path().cwd().parent / \"data\"\n",
    "export_path = data_dir / f\"{stack}\"\n",
    "\n",
    "if not export_path.exists(): \n",
    "    export_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd6a17e-a4bd-49a2-b064-2eb17318700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)\n",
    "shutil.unpack_archive(data_dir / file_name, export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995baa3-5c4a-48d5-9086-b9b8ce293808",
   "metadata": {},
   "source": [
    "# Step 2. \n",
    "\n",
    "Refactor the code from Chapter 10 of the book into a function that works for the `posts.xml`. That function should take in the base data and then:\n",
    "1. Convert the `int` variables (except where they start or end in `Id`) into integer values. \n",
    "2. Convert the `datetime` variables. \n",
    "3. Convert tags data from `str` into a `list`.\n",
    "4. Create a separate column for `CleanBody` which is the `Body` without HTML. \n",
    "5. Assign the HTML into a column as a list called `ListURL`.\n",
    "6. Pickle that DataFrame with a coherent name, such as `f\"{stack}_Posts_cleaned.xml\"`.\n",
    "\n",
    "Notes: \n",
    "> I say 'in a function', but you might want to have a main function and then helper functions for subprocesses. \n",
    "> You can make this more abstract, but that's coming anyway. Read the exercises below, and then think about this plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a9af143-9354-41c0-b597-c2605043329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Step 2 below here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd063980-d98e-4550-b195-2d8e098e6a1a",
   "metadata": {},
   "source": [
    "# Step 3. \n",
    "\n",
    "Parameterise the function. Depending on how you created the function above, you might have hard coded the names of the columns from the `Posts.xml` data. This time, make a parameter for the specific schema that you are going to use to convert the data. The schema should be stored as JSON and have the type of XML file as a key, with the value being a dictionary for the column names and the conversion, such as the following: \n",
    "\n",
    "~~~ Python\n",
    "[\"Posts\": {\"Id\":none,\n",
    "           \"PostTypeID\":none,...,\n",
    "           \"Tags\":[\"str\",\"list\"],\n",
    "           \"AnswerCount\":\"int\",\n",
    "           \"Body\":[\"str\",\"CleanHTML\",\"listHTML\"]},\n",
    " \"Users\": {\"Id\":none,\n",
    "           \"Reputation\":\"int\", ...}\n",
    "~~~\n",
    "\n",
    "So, now this time, the main function should read in the Schema from file, select the right table type (such as \"Posts\") and then return (or export to pickle) a DataFrame with the same naming conventions as above.   \n",
    "\n",
    "Full schema available here: https://i.stack.imgur.com/AyIkW.png\n",
    "\n",
    "> Note that this schema or question does not make assumptions about what you will do with None values, but it is encouraged for you to consider whether to convert to None, \"\", pd.NA, np.NAN, depending on context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82483988-d134-46e9-875c-37233ac89e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Step 3 below here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b515c26-ff72-4305-9fe5-c43d21aa7231",
   "metadata": {},
   "source": [
    "# Step 4. A wide parameter. \n",
    "\n",
    "Create a means (either with a parameter in the original file or using a separate file) in order to create a long table from one of the wide tables. That is, it should take the column that is used for the long data (which we assume would be a list of values) and then `explode()` the table. By default, it should only explode the selected column. You should be able to pass it a list of additional columns that will also appear in the exploded data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2bd45-c7ae-444e-a391-9c733e7d32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Step 4 below here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b93c31-e261-459d-b548-d3809b1b95af",
   "metadata": {},
   "source": [
    "# Challenge #2. \n",
    "_(example code will not be provided)_\n",
    "\n",
    "Recall that we downloaded the data from the Internet Archive. That main URL has a list of all of the Stack Exchanges and their sizes. \n",
    "\n",
    "Can we use this data in order to present a list of Stack Exchanges and then have the user select which one to first download instead of linking directly? \n",
    "\n",
    "Explore this as well as some packages for providing progress bars for the download process. Package all the code up in a script that allows the user to select which Exchange and then receive the resulting preferred tables as .pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92e4e6-89ec-43ba-87da-6d64286e4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Challenge 2 below here \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
